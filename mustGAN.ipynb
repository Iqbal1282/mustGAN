{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch \nimport torch.nn as nn \nimport matplotlib.pyplot as plt \nimport torch.nn.functional as F \nimport torch.optim as optim \ntorch.backends.cudnn.benchmark = True\nimport albumentations as A\nfrom torch.utils.data import Dataset, DataLoader  \nimport os \nfrom PIL import Image \nfrom albumentations.pytorch import ToTensorV2\nfrom torchvision.utils import save_image\nimport tqdm\nimport cv2 \nimport numpy as np \n\nfrom skimage.metrics import structural_similarity\nfrom skimage.metrics import peak_signal_noise_ratio","metadata":{"execution":{"iopub.status.busy":"2022-06-14T10:36:51.253076Z","iopub.execute_input":"2022-06-14T10:36:51.253622Z","iopub.status.idle":"2022-06-14T10:36:51.260179Z","shell.execute_reply.started":"2022-06-14T10:36:51.253582Z","shell.execute_reply":"2022-06-14T10:36:51.259407Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"t1_actual = '../input/ixi-t1/image slice-T1'\nt1_generated = '../input/t1-generated-22-must/t1_generated_22_MUST'","metadata":{"execution":{"iopub.status.busy":"2022-06-14T10:36:05.464263Z","iopub.execute_input":"2022-06-14T10:36:05.465025Z","iopub.status.idle":"2022-06-14T10:36:05.471158Z","shell.execute_reply.started":"2022-06-14T10:36:05.464989Z","shell.execute_reply":"2022-06-14T10:36:05.470503Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Discriminator","metadata":{}},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, in_channels = 1, features = [32,64, 128, 256, 512]):\n        super(Discriminator, self).__init__()\n        \n        self.initial = nn.Sequential(\n            nn.Conv2d(in_channels*2, \n            features[0],\n            kernel_size = 4, \n            stride = 2, \n            padding = 1, \n            padding_mode = 'reflect')\n        )\n        layers = []\n        \n        in_channel = features[0]\n        for out_channel in features[1:]:\n            layer = self.block(in_channel, out_channel,stride = 1 if out_channel == features[-1] else 2 )\n            in_channel = out_channel \n            layers.append(layer)\n            \n        layers.append(\n            nn.Conv2d(\n                in_channel, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"\n            ),\n        )\n\n            \n        self.model = nn.Sequential(*layers)\n            \n            \n        \n    def block(self, in_channel, out_channel, stride = 1):\n        block_layer = nn.Sequential(\n            nn.Conv2d(in_channel, out_channel, 4, stride,1, bias = False, padding_mode = 'reflect'),\n            nn.BatchNorm2d(out_channel),\n            nn.LeakyReLU(0.2)\n        )\n        return block_layer \n    def forward(self, x,y):\n        x = torch.cat([x, y] , dim = 1)\n        x = self.initial(x)\n        x = self.model(x)\n        return x ","metadata":{"execution":{"iopub.status.busy":"2022-06-14T10:36:05.472449Z","iopub.execute_input":"2022-06-14T10:36:05.473140Z","iopub.status.idle":"2022-06-14T10:36:05.487006Z","shell.execute_reply.started":"2022-06-14T10:36:05.473039Z","shell.execute_reply":"2022-06-14T10:36:05.486183Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Many to one & one to one ","metadata":{}},{"cell_type":"code","source":"# Residual block\n# 3x3 convolution\ndef conv3x3(in_channels, out_channels, stride=1):\n    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n                     stride=stride, padding=1, bias=False)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = conv3x3(in_channels, out_channels, stride)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(out_channels, out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n        \n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n        return out\n\n\n\nclass Streams(nn.Module):\n    def __init__(self, input_channel = 50):\n        super(Streams, self).__init__()\n        e1 = nn.Conv2d(input_channel, 32, kernel_size=3, stride=2, padding=4, bias=False)\n        e2 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=0, bias=False)\n        e3 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=0, bias=False)\n        \n        self.encoder = nn.Sequential(\n            *[e1, e2, e3]         \n        )\n        \n        layers = []\n        for i in range(5):\n            layer = ResidualBlock(32,32,1)\n            layers.append(layer)\n            \n\n        self.residual_network_first_half = nn.Sequential(*layers)\n        \n        layers = []\n        for i in range(4):\n            layer = ResidualBlock(32,32,1)\n            layers.append(layer)\n            \n            \n        self.residual_network_second_half = nn.Sequential(*layers)\n        \n        \n        d1 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding= 1, bias=False)\n        d2 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding = 0, bias=False)\n        d3 = nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding = 0, bias=False)\n        \n        self.decoder = nn.Sequential(*[d1, d2, d3])\n        \n            \n    def forward(self, x, fuse = False):\n        \n        x = self.encoder(x)\n        encoder_out = x \n        #print(x.shape)\n        x = self.residual_network_first_half(x)\n        res_out_first = x\n        x = self.residual_network_second_half(x)\n        res_out_second = x\n        #print(x.shape)\n        x = self.decoder(x)\n        if not fuse:\n            return x \n        else: \n            return x , encoder_out, res_out_first, res_out_second","metadata":{"execution":{"iopub.status.busy":"2022-06-14T10:36:05.488855Z","iopub.execute_input":"2022-06-14T10:36:05.489357Z","iopub.status.idle":"2022-06-14T10:36:05.507761Z","shell.execute_reply.started":"2022-06-14T10:36:05.489322Z","shell.execute_reply":"2022-06-14T10:36:05.506999Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# MustGAN ","metadata":{}},{"cell_type":"code","source":"class MustGAN(nn.Module):\n    def __init__(self, num_slice = 50):\n        super(MustGAN, self).__init__()\n        \n        self.num_slice = num_slice \n        \n        self.one_one = Streams(input_channel = 1)\n        self.many_one = Streams(input_channel = 50)\n        \n        layers = []\n        for i in range(5):\n            if i ==0: \n                layer =  nn.Conv2d(1632, 512, kernel_size=3, stride=1, padding=1, bias=False)\n            else: \n                layer = ResidualBlock(512,512,1)\n            layers.append(layer)\n            \n        self.encoder = nn.Sequential(*layers)\n        \n        \n        layers = []\n        for i in range(4):\n            if i == 0: \n                layer =  nn.Conv2d(2144, 512, kernel_size=3, stride=1, padding=1, bias=False)\n            else: \n                layer = ResidualBlock(512,512,1)\n            layers.append(layer)\n            \n        self.residual = nn.Sequential(*layers)\n        \n        d1 = nn.ConvTranspose2d(2144, 1632, kernel_size=3, stride=2, padding= 1, bias=False)\n        d2 = nn.ConvTranspose2d(1632, 256, kernel_size=3, stride=2, padding = 0, bias=False)\n        d3 = nn.ConvTranspose2d(256, 50, kernel_size=4, stride=2, padding = 0, bias=False)\n        \n        self.decoder = nn.Sequential(*[d1, d2, d3])\n        \n        \n    def forward(self,x_bulk):\n        fuse1 = []\n        fuse2 = []\n        fuse3 = []\n        \n        for i in range(self.num_slice):\n            _ , encoder_out, res_out_first, res_out_second = self.one_one(x_bulk[:,i:i+1,:,:], fuse = True)\n            fuse1.append(encoder_out)\n            fuse2.append(res_out_first)\n            fuse3.append(res_out_second)\n            \n        _ , encoder_out, res_out_first, res_out_second = self.many_one(x_bulk, fuse = True)\n        \n        fuse1.append(encoder_out)\n        fuse2.append(res_out_first)\n        fuse3.append(res_out_second)\n        \n        fuse1 = torch.cat(fuse1, 1)\n        fuse2 = torch.cat(fuse2, 1)\n        fuse3 = torch.cat(fuse3, 1)\n        \n        x = self.encoder(fuse1)\n        x = torch.cat([x, fuse2],1)\n        x = self.residual(x)\n\n        x = torch.cat([x, fuse3],1)\n        x = self.decoder(x)\n        \n        #print(fuse1.shape, fuse2.shape, fuse3.shape)\n        return x\n        \n        \n            ","metadata":{"execution":{"iopub.status.busy":"2022-06-14T10:36:05.510589Z","iopub.execute_input":"2022-06-14T10:36:05.510972Z","iopub.status.idle":"2022-06-14T10:36:05.528389Z","shell.execute_reply.started":"2022-06-14T10:36:05.510884Z","shell.execute_reply":"2022-06-14T10:36:05.527544Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Training MustGAN","metadata":{}},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nTRAIN_DIR = \"dataset/train\"\nT1_PATH = '../input/ixi-t1'\nT2_PATH = '../input/ixit2-slices'\nVAL_DIR = \"dataset/val\"\nLEARNING_RATE = 2e-4\nBATCH_SIZE = 4\nNUM_WORKERS = 0\nIMAGE_SIZE = 256\nCHANNELS_IMG = 3\nL1_LAMBDA = 100\nLAMBDA_GP = 10\nNUM_EPOCHS = 500\nLOAD_MODEL = False\nSAVE_MODEL = False\nCHECKPOINT_DISC = \"must_dis.pth.tar\"\nCHECKPOINT_GEN = \"must_gan.pth.tar\"\n\n\nboth_transform = A.Compose(\n    [A.Resize(width=256, height=256),], additional_targets={\"image0\": \"image\"},\n)\n\ntransform_only_input = A.Compose(\n    [\n        A.HorizontalFlip(p=0.5),\n        A.ColorJitter(p=0.2),\n        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n        ToTensorV2(),\n    ]\n)\n\ntransform_only_mask = A.Compose(\n    [\n        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n        ToTensorV2(),\n    ])","metadata":{"execution":{"iopub.status.busy":"2022-06-14T10:36:05.531033Z","iopub.execute_input":"2022-06-14T10:36:05.533045Z","iopub.status.idle":"2022-06-14T10:36:05.599185Z","shell.execute_reply.started":"2022-06-14T10:36:05.533010Z","shell.execute_reply":"2022-06-14T10:36:05.598263Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import os \nimport shutil\n\n\nt1_list = os.listdir(\"../input/ixi-t1/image slice-T1\")\nt2_list  = os.listdir(\"../input/ixit2-slices/image slice-T2\")\n\nprint(len(t1_list))\nprint(len(t2_list))\n\nt1_sampled = []\nt2_sampled = []\n\nfor file1 in t1_list:\n    found = False \n    for file2 in t2_list:\n        if file1.split(\"-\")[0] == file2.split(\"-\")[0]:\n            found = True \n            match_folder = file2 \n\n    if found:  \n        #print(\"matched\")\n        t1_sampled.append(file1)\n        t2_sampled.append(match_folder)\n        #print(file1 +\"\\t\"+ match_folder)\n\n    else: \n        #shutil.rmtree(\"./image slice-T2/\"+file1)\n        print(\"no matched\")","metadata":{"execution":{"iopub.status.busy":"2022-06-14T10:36:06.134545Z","iopub.execute_input":"2022-06-14T10:36:06.135316Z","iopub.status.idle":"2022-06-14T10:36:06.726568Z","shell.execute_reply.started":"2022-06-14T10:36:06.135278Z","shell.execute_reply":"2022-06-14T10:36:06.725820Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(len(t1_sampled))\nprint(len(t2_sampled))","metadata":{"execution":{"iopub.status.busy":"2022-06-14T10:36:06.892030Z","iopub.execute_input":"2022-06-14T10:36:06.892375Z","iopub.status.idle":"2022-06-14T10:36:06.897549Z","shell.execute_reply.started":"2022-06-14T10:36:06.892347Z","shell.execute_reply":"2022-06-14T10:36:06.896773Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class MapDataset(Dataset):\n    def __init__(self,root_dir_T1, root_dir_T2, t1_sampled, t2_sampled):\n        super(MapDataset, self).__init__() \n        self.list_files_t1 = t1_sampled\n        self.list_files_t2 = t2_sampled\n        \n        self.root_dir_T1 = root_dir_T1\n        self.root_dir_T2 = root_dir_T2\n    def __len__(self):\n        return len(self.list_files_t1)\n    \n    def __getitem__(self, idex):\n        folder_file1 = self.list_files_t1[idex]\n        image_files = os.listdir(os.path.join(self.root_dir_T1, folder_file1))\n        \n        out_file1 = np.zeros((50, 256, 256))\n        for i, imf in enumerate(image_files): \n            path_file = os.path.join(self.root_dir_T1, folder_file1, imf)\n            img = cv2.imread(path_file, 0)\n            img = cv2.resize(img, (256, 256))\n            #img = np.expand_dims(img,0)\n            out_file1[i] = img \n            \n        folder_file2 = self.list_files_t2[idex]\n        image_files2 = os.listdir(os.path.join(self.root_dir_T2, folder_file2))\n        \n        out_file2 = np.zeros((50, 256, 256))\n        for i, imf in enumerate(image_files2): \n            path_file = os.path.join(self.root_dir_T2, folder_file2, imf)\n            img = cv2.imread(path_file, 0)\n            img = cv2.resize(img, (256, 256))\n            #img = np.expand_dims(img,0)\n            out_file2[i] = img \n            \n            \n        \n        return out_file1.astype(np.float32), out_file2.astype(np.float32)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T10:36:08.575862Z","iopub.execute_input":"2022-06-14T10:36:08.576574Z","iopub.status.idle":"2022-06-14T10:36:08.587009Z","shell.execute_reply.started":"2022-06-14T10:36:08.576537Z","shell.execute_reply":"2022-06-14T10:36:08.586253Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"root_dir_T1 = '../input/ixi-t1/image slice-T1'\nroot_dir_T2 = '../input/ixit2-slices/image slice-T2'\ndataset = MapDataset(root_dir_T1, root_dir_T2, t1_sampled, t2_sampled)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T10:36:11.085343Z","iopub.execute_input":"2022-06-14T10:36:11.086223Z","iopub.status.idle":"2022-06-14T10:36:11.091865Z","shell.execute_reply.started":"2022-06-14T10:36:11.086179Z","shell.execute_reply":"2022-06-14T10:36:11.090647Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndisc = Discriminator(in_channels= 50).to(device)\ngen = MustGAN().to(device)\nopt_disc = optim.Adam(disc.parameters(), lr = 0.0002, betas = (0.5, 0.999))\nopt_gen = optim.Adam(gen.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n\nBCE = nn.BCEWithLogitsLoss()\nL1_LOSS = nn.L1Loss()","metadata":{"execution":{"iopub.status.busy":"2022-06-14T10:36:12.448152Z","iopub.execute_input":"2022-06-14T10:36:12.448493Z","iopub.status.idle":"2022-06-14T10:36:16.218509Z","shell.execute_reply.started":"2022-06-14T10:36:12.448466Z","shell.execute_reply":"2022-06-14T10:36:16.217616Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(\n    dataset, \n    batch_size = BATCH_SIZE,\n    shuffle = True,\n    num_workers = NUM_WORKERS\n)\n\nval_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n\ng_scaler = torch.cuda.amp.GradScaler()\nd_scaler = torch.cuda.amp.GradScaler()","metadata":{"execution":{"iopub.status.busy":"2022-06-14T10:36:16.220008Z","iopub.execute_input":"2022-06-14T10:36:16.220523Z","iopub.status.idle":"2022-06-14T10:36:16.227506Z","shell.execute_reply.started":"2022-06-14T10:36:16.220484Z","shell.execute_reply":"2022-06-14T10:36:16.226584Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def train_fn(disc, gen, loader, opt_disc, opt_gen, l1_loss, bce, g_scaler, d_scaler):\n    pbar = tqdm.tqdm(loader, leave = True)\n    for idx,(x,y) in enumerate(pbar):\n        #print(x.shape)\n        #print(y.shape)\n        x = x.to(device) # input image type \n        y = y.to(device) # target image type \n\n        # train discriminator \n        with torch.cuda.amp.autocast():\n            #print(x.dtype)\n            y_fake = gen(x) # fake target generation\n            \n            D_real = disc(x,y) # disc pred with actual image \n            D_real_loss = bce(D_real, torch.ones_like(D_real))\n\n            D_fake = disc(x, y_fake.detach())\n            D_fake_loss = bce(D_fake, torch.zeros_like(D_fake))\n\n            D_loss = (D_fake_loss + D_real_loss)/2 \n\n        opt_disc.zero_grad()\n        # D_loss.backward()\n        # opt_disc.step()\n        d_scaler.scale(D_loss).backward()\n        d_scaler.step(opt_disc)\n        d_scaler.update()\n\n        # train generator \n        with torch.cuda.amp.autocast():\n            D_fake = disc(x, y_fake)\n            G_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n            L1 = l1_loss(y_fake,y)*L1_LAMBDA\n            G_loss = G_fake_loss + L1\n\n        opt_gen.zero_grad()\n        g_scaler.scale(G_loss).backward()\n        g_scaler.step(opt_gen)\n        g_scaler.update()\n\n        if idx % 10 == 0:\n            pbar.set_postfix(\n                D_real = torch.sigmoid(D_real).mean().item(),\n                D_fake = torch.sigmoid(D_fake).mean().item(),\n            )\n        ","metadata":{"execution":{"iopub.status.busy":"2022-06-14T10:36:16.228810Z","iopub.execute_input":"2022-06-14T10:36:16.229145Z","iopub.status.idle":"2022-06-14T10:36:16.242575Z","shell.execute_reply.started":"2022-06-14T10:36:16.229109Z","shell.execute_reply":"2022-06-14T10:36:16.241823Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def save_some_examples(gen, val_loader, epoch, folder):\n    x, y = next(iter(val_loader))\n    x, y = x.to(DEVICE), y.to(DEVICE)\n    gen.eval()\n    if not os.path.exists(folder):\n        os.mkdir(folder)\n    with torch.no_grad():\n        y_fake = gen(x)\n        y_fake = y_fake * 0.5 + 0.5  # remove normalization#\n        \n        save_image(y_fake[:,0:1,:,:], folder + f\"/y_gen_{epoch}.png\")\n        save_image(x[:,0:1,:,:] * 0.5 + 0.5, folder + f\"/input_{epoch}.png\")\n        save_image(y[:,0:1,:,:] * 0.5 + 0.5, folder + f\"/label_{epoch}.png\")\n    gen.train()\n\n\ndef save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    torch.save(checkpoint, filename)\n\n\ndef load_checkpoint(checkpoint_file, model, optimizer, lr):\n    print(\"=> Loading checkpoint\")\n    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n    # If we don't do this then it will just have learning rate of old checkpoint\n    # and it will lead to many hours of debugging \\:\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr","metadata":{"execution":{"iopub.status.busy":"2022-06-14T10:36:16.243997Z","iopub.execute_input":"2022-06-14T10:36:16.244502Z","iopub.status.idle":"2022-06-14T10:36:16.257097Z","shell.execute_reply.started":"2022-06-14T10:36:16.244461Z","shell.execute_reply":"2022-06-14T10:36:16.256282Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"SAVE_MODE = True \n\nfor epoch in range(NUM_EPOCHS):\n    train_fn(disc, gen, train_loader, opt_disc, opt_gen, L1_LOSS, BCE, g_scaler, d_scaler)\n    if SAVE_MODE and epoch %1 ==0: \n        save_checkpoint(gen, opt_gen, filename = CHECKPOINT_GEN)\n        save_checkpoint(disc, opt_disc, filename = CHECKPOINT_DISC)\n            \n    save_some_examples(gen, val_loader, epoch, folder = 'evaluation')   ","metadata":{"execution":{"iopub.status.busy":"2022-06-14T08:06:08.933408Z","iopub.execute_input":"2022-06-14T08:06:08.933959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PSNR and SSIM ","metadata":{}},{"cell_type":"code","source":"PSNR = []\nSSIM = []\n\n\nfor sub_folder in tqdm.tqdm(t1_sampled):\n    for file in os.listdir(os.path.join(t1_actual,sub_folder)):\n        \n        img_actual = cv2.imread(os.path.join(t1_actual, sub_folder, file))\n        img_generated = cv2.imread(os.path.join(t1_generated, sub_folder, file))\n        #print(os.path.join(t1_generated, sub_folder, file))\n        #print(os.path.join(t1_actual, sub_folder, file))\n        \n        img_actual = cv2.resize(img_actual, (256, 256))\n        img_generated = cv2.resize(img_generated, (256, 256))\n        #print(img_actual.shape)\n        #print(img_generated.shape)\n       \n        psnr = peak_signal_noise_ratio(img_actual, img_generated)\n        ssim = structural_similarity(img_actual, img_generated,multichannel=True)\n        \n        PSNR.append(psnr)\n        SSIM.append(ssim)\n        \nprint(np.mean(PSNR))\nprint(np.mean(SSIM))","metadata":{"execution":{"iopub.status.busy":"2022-06-14T10:36:59.094674Z","iopub.execute_input":"2022-06-14T10:36:59.095219Z","iopub.status.idle":"2022-06-14T10:49:50.762612Z","shell.execute_reply.started":"2022-06-14T10:36:59.095179Z","shell.execute_reply":"2022-06-14T10:49:50.761836Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"count = 0 \noutput= \"./\"\nfor sub_folder in os.listdir(t1_actual):\n    if count==9:\n        break\n    count+=1\n    \n    \n    for file in os.listdir(os.path.join(t1_actual,sub_folder)):\n        \n        img_actual = cv2.imread(os.path.join(t1_actual, sub_folder, file))\n        img_generated = cv2.imread(os.path.join(output,t1_generated, sub_folder, file))\n        #print(os.path.join(t1_generated, sub_folder, file))\n        #print(os.path.join(t1_actual, sub_folder, file))\n        \n        img_actual = cv2.resize(img_actual, (256, 256))\n        img_generated = cv2.resize(img_generated, (256, 256))\n        #print(img_actual.shape)\n        #print(img_generated.shape)\n        \n        plt.subplot(121)\n        plt.imshow(img_actual)\n        \n        plt.subplot(122)\n        plt.imshow(img_generated)\n        plt.title(\"Actual Vs Generated\")\n        plt.show()\n        \n        \n        break","metadata":{"execution":{"iopub.status.busy":"2022-06-14T10:52:53.206084Z","iopub.execute_input":"2022-06-14T10:52:53.206638Z","iopub.status.idle":"2022-06-14T10:52:55.950301Z","shell.execute_reply.started":"2022-06-14T10:52:53.206599Z","shell.execute_reply":"2022-06-14T10:52:55.949439Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}